---
layout: post
title: "Classification Model of Movie Rating Prior to Being Released"
---

## Predicting Movie Ratings from IMDB Top 250

What factors determine a high-rating for a movie? This analysis investigates features that can predict whether a movie will be a high or low rating prior to being released. Under this scenario, there is a market where streaming services can buy movies prior to being released, essentially buying low before the movie's performance is realized. If the transaction costs are high, then the optimal move would be to limit Type I error, that is minimizing false positives. False Positives are when the model predicts an outcome (such as True/1) when it is not (False/0). This is analogous to buying "lemons". If the transaction costs are low, then minimizing Type II error (False Negatives) would be the strategy. False Negatives are when the model predicts False/0 when it is actually True/1. This scenario will fall under the former category, minimizing Type I error.

Using code to scrape IMDB website, movies will be scraped to collect information about the movies. Another scrape will go to each movie's url on IMDB to scrape budget, production companies, creators etc.. The code for the main scrape came from Amer Shalan, which can be found [[here]()

The jupyter notebook can be found [here.](https://github.com/adalal80/GA-DSI/blob/master/projects/projects-weekly/project-06/Project%206%20-%20IMDB.ipynb)

### Data

The main scrape was for approximately 10,000 movies, and then it was filtered by movies released in the year 2000 and later. Then those movies were scraped to collect information on budget, rating, production companies, and creators. Only those movies with positive budgets will be used in the model.

The features used in the models are:
* Runtime
* Budget
* Actors
* Directors
* Genres
* Source (Screenplay/Novel)

The next section will talk about feature engineering

### Feature Engineering

#### Actors
Actors were binned into the following categories: super actors, top actors, mid actors, bottom actors. In this first iteration, the above will be used. In later iterations, time effects need to be involved, because a top actor in early 2000s, but not necessarily the top actor now. Leonardo DiCaprio is a top actor now, but not in the early 2000s.

#### Directors
Directors were binned into the following categories: Super Director, Top Director, Mid Director.  In this iteration, the above will be used. In the later iterations, time effects will need to be involved just like for actors.

#### Production Companies
Production Companies was binned into Big Six or not. The Big Six companies are:
* Warner Bros.
* Columbia Pictures (Sony)
* Walt Disney Studios
* Universal
* Paramount
* Twentieth Century Fox

#### Novel/Screenplay
Movies were binned into:
* Novel
* Screenplay

There were other types such as books and comics.

#### Genre
There were many genres such as Drama, Comedy, Action, Adventure, Thriller etc... Many overlapped, so the two dominant genres were used:
* Drama
* Comedy

The other genres were all grouped together, and omitted.

### Models

* Decision Tree

* Random Forest

* Extra Trees

* Gradient Boosting


### Results
Iterating through ratings between 6.4 (median and mean) and 8.0, it was found that 7.4 resulted in the highest precision for Random Forest Model. At a rating of 7.2, the Random Forest model has a slightly worse precision and a higher recall score, but also that is the rating where Gradient Boosting model precision score is maximized. Depending on whether minimizing Type I or Type II error is the goal, different ratings might be used to split. Since Type I error is going to be minimized, rating will be split at 7.2. 




Runtime and Budget were the two most important features. Using cross val scores for Decision Trees, Random Forest, and Extra Trees, the best model yielded Decision Trees, followed by Random Forest.

The F1-Score and precision score (predicting high rating given it was a high rated movie) for Decision Trees were: 68% and 67%. The same scores for Random Forests were 62% and 58%. The following is the ROC curve for Decision Trees model. This plots the relationship between True Positives and False Positives for different thresholds.

![ROC](https://github.com/adalal80/adalal80.github.io/blob/master/images/Project6_ROC_DT.png?raw=true)


### Conclusion

This analysis investigated the features of what makes a good movie (high rating). Since the access rights to movies do cost companies like Netflix, it is important that they select movies that their users want to watch, otherwise, they might shift to competitors.

The analysis here was not robust enough, as more than top250 movies are needed to determine whether the features can predict the movie rating. There are some unobservables such as plot. One can assume that being nominated for awards is a decend indicator, but there are observations of movies that are blockbusters, but have not been nominated or won an Oscar.

Collecting more data should improve this model and thus predict whether a movie will have a high rating. One note to point out is that we need to include which movies are on netflix and other streaming site, as that would increase the number of sources that a movie could be watched. Also ratings might be biased, as it could be only certain type of people would take time to rate a movie.